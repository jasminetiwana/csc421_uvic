{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Fall 2021 Assignment 4 \n",
    "### Author: George Tzanetakis \n",
    "\n",
    "This notebook is based on the topics covered in **Chapter 14 - Probabilistic Reasoning over Time**, **Chapter 20 - Learning probabilistic models**, and **Chapter 19 Learning from Examples** from the book *Artificial Intelligence: A Modern Approach.*  You are welcome and actually it can be educational to look at the code at the aima-code repository as well as other code resources you can find on the web. However, make sure you understand any code that you incoporate. \n",
    "\n",
    "The assignment structure is as follows - each item is worth 1 point: \n",
    "\n",
    "1. Bayesian Network  (Basic) - express network and print CPT  \n",
    "2. Bayesian Network  (Expected) - markdown and direct inference   \n",
    "3. Bayesian Network  (Basic) -  approximate inference (rejection sampling and likelihood weighting) \n",
    "4. Bayesian Netowrk  (Advanced) - naive bayes of movie reviews as bayesian network \n",
    "5. Hidden Markov Models (Basic) - Use HMM to generate plausible DNA sequences and visualize \n",
    "6. Hidden Markov Models (Expected) - Learn HMM from samples for DNA sequences \n",
    "7. Hidden Markov Model (Expected) - Compare classification accuracy of ignoring transition matrix \n",
    "8. Hidden Markov Models (Advanced) - make up HMM scenario for activity detection using 2D coordinates and GMMs  \n",
    "9. Classification (Basic) - Replicate movie review classification using bernoulli Naive Bayes in sklearn \n",
    "10. Classification(Expected) - Explore a standard classification problem with continuous attributes in sklearn \n",
    "\n",
    "The grading will be done in 0.5 increments. 1 point for correct answer, 0.5 points for partial or incorrect \n",
    "but reasonable answer and 0.0 for no answer or completely wrong answer. \n",
    "\n",
    "**Misunderstanding of probability may be the greatest of all impediments\n",
    "to scientific literacy.** \n",
    "\n",
    "**Gould, Stephen Jay** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (Basic)  - 1 point\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"dispnea.png\">\n",
    "\n",
    "Using the convetions for DBNs used in probability.ipynb (from the AIMA authors) encode the diapnea network shown above. Once you have constructed the Bayesian network display the cpt for the Lung Cancer Node (using the API provided not just showing the numbers).\n",
    "\n",
    "The cell below contains the code that defined BayesNodes and BayesNetworks and the following cell \n",
    "shows an example of defining the Burglary network and performing a query using direct enumeration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.21.4)\n",
      "Requirement already satisfied: hmmlearn in /opt/conda/lib/python3.9/site-packages (0.2.6)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.21.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.9/site-packages (from scipy) (1.21.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (6.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.28.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.21.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install hmmlearn\n",
    "!pip3 install scipy\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "\n",
    "def extend(s, var, val):\n",
    "    \"\"\"Copy dict s and extend it by setting var to val; return copy.\"\"\"\n",
    "    return {**s, var: val}\n",
    "\n",
    "def event_values(event, variables):                                                                      \n",
    "    \"\"\"Return a tuple of the values of variables in event.                                               \n",
    "    >>> event_values ({'A': 10, 'B': 9, 'C': 8}, ['C', 'A'])                                             \n",
    "    (8, 10)                                                                                              \n",
    "    >>> event_values ((1, 2), ['C', 'A'])                                                                \n",
    "    (1, 2)                                                                                               \n",
    "    \"\"\"                                                                                                  \n",
    "    if isinstance(event, tuple) and len(event) == len(variables):                                        \n",
    "        return event                                                                                     \n",
    "    else:                                                                                                \n",
    "        return tuple([event[var] for var in variables])                                                  \n",
    "                      \n",
    "def probability(p):                                                                                      \n",
    "    \"\"\"Return true with probability p.\"\"\"                                                                \n",
    "    return p > random.uniform(0.0, 1.0)  \n",
    "        \n",
    "class ProbDist:\n",
    "    \"\"\"A discrete probability distribution. You name the random variable\n",
    "    in the constructor, then assign and query probability of values.\n",
    "    >>> P = ProbDist('Flip'); P['H'], P['T'] = 0.25, 0.75; P['H']\n",
    "    0.25\n",
    "    >>> P = ProbDist('X', {'lo': 125, 'med': 375, 'hi': 500})\n",
    "    >>> P['lo'], P['med'], P['hi']\n",
    "    (0.125, 0.375, 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var_name='?', freq=None):\n",
    "        \"\"\"If freq is given, it is a dictionary of values - frequency pairs,\n",
    "        then ProbDist is normalized.\"\"\"\n",
    "        self.prob = {}\n",
    "        self.var_name = var_name\n",
    "        self.values = []\n",
    "        if freq:\n",
    "            for (v, p) in freq.items():\n",
    "                self[v] = p\n",
    "            self.normalize()\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        \"\"\"Given a value, return P(value).\"\"\"\n",
    "        try:\n",
    "            return self.prob[val]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def __setitem__(self, val, p):\n",
    "        \"\"\"Set P(val) = p.\"\"\"\n",
    "        if val not in self.values:\n",
    "            self.values.append(val)\n",
    "        self.prob[val] = p\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Make sure the probabilities of all values sum to 1.\n",
    "        Returns the normalized distribution.\n",
    "        Raises a ZeroDivisionError if the sum of the values is 0.\"\"\"\n",
    "        total = sum(self.prob.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            for val in self.prob:\n",
    "                self.prob[val] /= total\n",
    "        return self\n",
    "\n",
    "    def show_approx(self, numfmt='{:.3g}'):\n",
    "        \"\"\"Show the probabilities rounded and sorted by key, for the\n",
    "        sake of portable doctests.\"\"\"\n",
    "        return ', '.join([('{}: ' + numfmt).format(v, p) for (v, p) in sorted(self.prob.items())])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"P({})\".format(self.var_name)\n",
    "\n",
    "\n",
    "class BayesNode:\n",
    "    \"\"\"A conditional probability distribution for a boolean variable,\n",
    "    P(X | parents). Part of a BayesNet.\"\"\"\n",
    "\n",
    "    def __init__(self, X, parents, cpt):\n",
    "        \"\"\"X is a variable name, and parents a sequence of variable\n",
    "        names or a space-separated string. cpt, the conditional\n",
    "        probability table, takes one of these forms:\n",
    "\n",
    "        * A number, the unconditional probability P(X=true). You can\n",
    "          use this form when there are no parents.\n",
    "\n",
    "        * A dict {v: p, ...}, the conditional probability distribution\n",
    "          P(X=true | parent=v) = p. When there's just one parent.\n",
    "\n",
    "        * A dict {(v1, v2, ...): p, ...}, the distribution P(X=true |\n",
    "          parent1=v1, parent2=v2, ...) = p. Each key must have as many\n",
    "          values as there are parents. You can use this form always;\n",
    "          the first two are just conveniences.\n",
    "\n",
    "        In all cases the probability of X being false is left implicit,\n",
    "        since it follows from P(X=true).\n",
    "\n",
    "        >>> X = BayesNode('X', '', 0.2)\n",
    "        >>> Y = BayesNode('Y', 'P', {T: 0.2, F: 0.7})\n",
    "        >>> Z = BayesNode('Z', 'P Q',\n",
    "        ...    {(T, T): 0.2, (T, F): 0.3, (F, T): 0.5, (F, F): 0.7})\n",
    "        \"\"\"\n",
    "        if isinstance(parents, str):\n",
    "            parents = parents.split()\n",
    "\n",
    "        # We store the table always in the third form above.\n",
    "        if isinstance(cpt, (float, int)):  # no parents, 0-tuple\n",
    "            cpt = {(): cpt}\n",
    "        elif isinstance(cpt, dict):\n",
    "            # one parent, 1-tuple\n",
    "            if cpt and isinstance(list(cpt.keys())[0], bool):\n",
    "                cpt = {(v,): p for v, p in cpt.items()}\n",
    "\n",
    "        assert isinstance(cpt, dict)\n",
    "        for vs, p in cpt.items():\n",
    "            assert isinstance(vs, tuple) and len(vs) == len(parents)\n",
    "            assert all(isinstance(v, bool) for v in vs)\n",
    "            assert 0 <= p <= 1\n",
    "\n",
    "        self.variable = X\n",
    "        self.parents = parents\n",
    "        self.cpt = cpt\n",
    "        self.children = []\n",
    "\n",
    "    def p(self, value, event):\n",
    "        \"\"\"Return the conditional probability\n",
    "        P(X=value | parents=parent_values), where parent_values\n",
    "        are the values of parents in event. (event must assign each\n",
    "        parent a value.)\n",
    "        >>> bn = BayesNode('X', 'Burglary', {T: 0.2, F: 0.625})\n",
    "        >>> bn.p(False, {'Burglary': False, 'Earthquake': True})\n",
    "        0.375\"\"\"\n",
    "        assert isinstance(value, bool)\n",
    "        ptrue = self.cpt[event_values(event, self.parents)]\n",
    "        return ptrue if value else 1 - ptrue\n",
    "\n",
    "    def sample(self, event):\n",
    "        \"\"\"Sample from the distribution for this variable conditioned\n",
    "        on event's values for parent_variables. That is, return True/False\n",
    "        at random according with the conditional probability given the\n",
    "        parents.\"\"\"\n",
    "        return probability(self.p(True, event))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.variable, ' '.join(self.parents)))\n",
    "    \n",
    "    \n",
    "class BayesNet:\n",
    "    \"\"\"Bayesian network containing only boolean-variable nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, node_specs=None):\n",
    "        \"\"\"Nodes must be ordered with parents before children.\"\"\"\n",
    "        self.nodes = []\n",
    "        self.variables = []\n",
    "        node_specs = node_specs or []\n",
    "        for node_spec in node_specs:\n",
    "            self.add(node_spec)\n",
    "\n",
    "    def add(self, node_spec):\n",
    "        \"\"\"Add a node to the net. Its parents must already be in the\n",
    "        net, and its variable must not.\"\"\"\n",
    "        node = BayesNode(*node_spec)\n",
    "        assert node.variable not in self.variables\n",
    "        assert all((parent in self.variables) for parent in node.parents)\n",
    "        self.nodes.append(node)\n",
    "        self.variables.append(node.variable)\n",
    "        for parent in node.parents:\n",
    "            self.variable_node(parent).children.append(node)\n",
    "\n",
    "    def variable_node(self, var):\n",
    "        \"\"\"Return the node for the variable named var.\n",
    "        >>> burglary.variable_node('Burglary').variable\n",
    "        'Burglary'\"\"\"\n",
    "        for n in self.nodes:\n",
    "            if n.variable == var:\n",
    "                return n\n",
    "        raise Exception(\"No such variable: {}\".format(var))\n",
    "\n",
    "    def variable_values(self, var):\n",
    "        \"\"\"Return the domain of var.\"\"\"\n",
    "        return [True, False]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BayesNet({0!r})'.format(self.nodes)\n",
    "    \n",
    "    \n",
    "def enumerate_all(variables, e, bn):\n",
    "    \"\"\"Return the sum of those entries in P(variables | e{others})\n",
    "    consistent with e, where P is the joint distribution represented\n",
    "    by bn, and e{others} means e restricted to bn's other variables\n",
    "    (the ones other than variables). Parents must precede children in variables.\"\"\"\n",
    "    if not variables:\n",
    "        return 1.0\n",
    "    Y, rest = variables[0], variables[1:]\n",
    "    Ynode = bn.variable_node(Y)\n",
    "    if Y in e:\n",
    "        return Ynode.p(e[Y], e) * enumerate_all(rest, e, bn)\n",
    "    else:\n",
    "        return sum(Ynode.p(y, e) * enumerate_all(rest, extend(e, Y, y), bn)\n",
    "                   for y in bn.variable_values(Y))\n",
    "\n",
    "def enumeration_ask(X, e, bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.9]\n",
    "    Return the conditional probability distribution of variable X\n",
    "    given evidence e, from BayesNet bn.\n",
    "    >>> enumeration_ask('Burglary', dict(JohnCalls=T, MaryCalls=T), burglary\n",
    "    ...  ).show_approx()\n",
    "    'False: 0.716, True: 0.284'\"\"\"\n",
    "    assert X not in e, \"Query variable must be distinct from evidence\"\n",
    "    Q = ProbDist(X)\n",
    "    for xi in bn.variable_values(X):\n",
    "        Q[xi] = enumerate_all(bn.variables, extend(e, X, xi), bn)\n",
    "    return Q.normalize()\n",
    "\n",
    "def consistent_with(event, evidence):\n",
    "    \"\"\"Is event consistent with the given evidence?\"\"\"\n",
    "    return all(evidence.get(k, v) == v for k, v in event.items())\n",
    "\n",
    "def prior_sample(bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.13]\n",
    "    Randomly sample from bn's full joint distribution.\n",
    "    The result is a {variable: value} dict.\n",
    "    \"\"\"\n",
    "    event = {}\n",
    "    for node in bn.nodes:\n",
    "        event[node.variable] = node.sample(event)\n",
    "    return event\n",
    "\n",
    "def rejection_sampling(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.14]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn, using N samples.\n",
    "    Raises a ZeroDivisionError if all the N samples are rejected,\n",
    "    i.e., inconsistent with e.\n",
    "    >>> random.seed(47)\n",
    "    >>> rejection_sampling('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.7, True: 0.3'\n",
    "    \"\"\"\n",
    "    counts = {x: 0 for x in bn.variable_values(X)}  # bold N in [Figure 14.14]\n",
    "    for j in range(N):\n",
    "        sample = prior_sample(bn)  # boldface x in [Figure 14.14]\n",
    "        if consistent_with(sample, e):\n",
    "            counts[sample[X]] += 1\n",
    "    return ProbDist(X, counts)\n",
    "\n",
    "def weighted_sample(bn, e):\n",
    "    \"\"\"\n",
    "    Sample an event from bn that's consistent with the evidence e;\n",
    "    return the event and its weight, the likelihood that the event\n",
    "    accords to the evidence.\n",
    "    \"\"\"\n",
    "    w = 1\n",
    "    event = dict(e)  # boldface x in [Figure 14.15]\n",
    "    for node in bn.nodes:\n",
    "        Xi = node.variable\n",
    "        if Xi in e:\n",
    "            w *= node.p(e[Xi], event)\n",
    "        else:\n",
    "            event[Xi] = node.sample(event)\n",
    "    return event, w\n",
    "\n",
    "def likelihood_weighting(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.15]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn.\n",
    "    >>> random.seed(1017)\n",
    "    >>> likelihood_weighting('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.702, True: 0.298'\n",
    "    \"\"\"\n",
    "    W = {x: 0 for x in bn.variable_values(X)}\n",
    "    for j in range(N):\n",
    "        sample, weight = weighted_sample(bn, e)  # boldface x, w in [Figure 14.15]\n",
    "        W[sample[X]] += weight\n",
    "    return ProbDist(X, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}\n",
      "0.2841718353643929 0.7158281646356071\n",
      "0.06666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False: 0.575, True: 0.425'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   burglary = BayesNet([\n",
    "        ('Burglary', '', 0.001),\n",
    "        ('Earthquake', '', 0.002),\n",
    "        ('Alarm', 'Burglary Earthquake',\n",
    "         {(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}),\n",
    "        ('JohnCalls', 'Alarm', {True: 0.90, False: 0.05}),\n",
    "        ('MaryCalls', 'Alarm', {True: 0.70, False: 0.01})\n",
    "    ])\n",
    "    \n",
    "print(burglary.variable_node('Alarm').cpt)\n",
    "ans_dist = enumeration_ask('Burglary', {'JohnCalls': True, 'MaryCalls': True}, burglary)\n",
    "print(ans_dist[True],ans_dist[False])\n",
    "\n",
    "\n",
    "p = rejection_sampling('Burglary', dict(JohnCalls=True, MaryCalls=True), burglary, 10000)\n",
    "print(p[True])\n",
    "\n",
    "likelihood_weighting('Burglary', dict(JohnCalls=True, MaryCalls=True),burglary, 10000).show_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True,): 0.1, (False,): 0.01}\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "dispnea = BayesNet([\n",
    "    ('A', '', 0.01), \n",
    "    ('S', '', 0.5), \n",
    "    ('T', 'A', {True: 0.05, False: 0.01}), \n",
    "    ('L', 'S', {True: 0.1, False: 0.01}), \n",
    "    ('B', 'S', {True: 0.6, False: 0.3}),\n",
    "    ('E', 'T L', {(True, True): 1, (True, False): 1, (False, True): 1, (False, False): 0}),\n",
    "    ('X', 'E', {True: 0.98, False: 0.05}),\n",
    "    ('D', 'E B', {(True, True): 0.9, (True, False): 0.7, (False, True): 0.8, (False, False): 0.1})\n",
    "])\n",
    "\n",
    "print(dispnea.variable_node('L').cpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (Expected) 1 point \n",
    "\n",
    "Answer using exact inference with enumeration the following query: given that a patient has been in Asia and has a positive xray, what is the likelihood of having dispnea?\n",
    "\n",
    "Write down using markdown the expression that corresponds to this query and the corresponding numbers from the CPT. There will be multiple sums and subscripts. Calculate the result using a calculator.\n",
    "\n",
    "Write code for the same query using enumeration_ask and confirm that the result is the same for the same query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6811011940658546\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "ans_dist = enumeration_ask('D', {'A': True, 'X': True}, dispnea)\n",
    "print(ans_dist[True])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(D|+X,+A) = P(+A) \\sum \\limits _{S}P(S) \\sum \\limits _{T}P(T|+A) \\sum \\limits _{L}P(L|S) \\sum \\limits _{E}P(E|T,L)P(+X|E) \\sum \\limits _{B}P(D|B,E) P(B|S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3 (Basic) - 1 point\n",
    "\n",
    "Answer using approximate inference the same query using both rejection sampling and likelihood weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection Sampling: 0.7333333333333333\n",
      "\n",
      "Likelihood Weighting "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False: 0.327, True: 0.673'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "p = rejection_sampling('D', dict(A=True, X=True), dispnea, 10000)\n",
    "print(\"Rejection Sampling\" + \":\" + \" \" + str(p[True]))\n",
    "print()\n",
    "print(\"Likelihood Weighting\", end=\" \")\n",
    "likelihood_weighting('D', dict(A=True, X=True), dispnea, 10000).show_approx()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4 (ADVANCED) - 1 point \n",
    "\n",
    "A Naive Bayes classifier can be considered as a Bayesian Network. The classification problem can then be expressed as setting all the variables corresponding to the features as evidence and querying the probability for the class. Express the Bernoulli Naive Bayes classifier you implemented in the previous assignment as a Bayesian Network using the probability.ipynb conventions used in this notebook. Now that you have a DBN express and solve the classification problem as a query and go over all the previous steps for this particular problem. More specifically do exact inference by enumeration, exact inference by variable elimination, approximate inference by rejection sampling and approximate inference by likelihood weighting to answer the query and show the results. Use 4 specific examples (2 positive and 2 negative) from the training dataset to show how the prediction using the Bayesian network works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971201036720275\n",
      "1.0\n",
      "0.7716358864478511\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c8ef08317a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mans_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menumeration_ask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'P'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Awful'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bad'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Boring'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dull'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Effective'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Enjoyable'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Great'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hilarious'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrejection_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'P'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAwful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEffective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnjoyable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGreat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHilarious\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-354d36d82442>\u001b[0m in \u001b[0;36mrejection_sampling\u001b[0;34m(X, e, bn, N)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconsistent_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mProbDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mweighted_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-354d36d82442>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_name, freq)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-354d36d82442>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "classifier = BayesNet([\n",
    "    ('P', '', 0.5), \n",
    "    ('Awful', 'P', {True: 0.019, False: 0.101}), \n",
    "    ('Bad', 'P', {True: 0.255, False: 0.505}), \n",
    "    ('Boring', 'P', {True: 0.048, False: 0.169}),\n",
    "    ('Dull', 'P', {True: 0.023, False: 0.091}),\n",
    "    ('Effective', 'P', {True: 0.12, False: 0.046}),\n",
    "    ('Enjoyable', 'P', {True: 0.095, False: 0.053}),\n",
    "    ('Great', 'P', {True: 0.408, False: 0.286}),\n",
    "    ('Hilarious', 'P', {True: 0.125, False: 0.05})\n",
    "])\n",
    "\n",
    "querypos1 = [0,0,0,0,1,1,1,1]\n",
    "querypos2 = [1,0,0,0,1,1,0,1]\n",
    "queryneg1 = [1,1,1,1,0,0,0,0]\n",
    "queryneg2 = [1,1,0,1,1,0,0,0]\n",
    "\n",
    "ans_dist = enumeration_ask('P', {'Awful': False, 'Bad': False, 'Boring': False, 'Dull': False, 'Effective': True, 'Enjoyable': True, 'Great': True, 'Hilarious': True}, classifier)\n",
    "print(ans_dist[True])\n",
    "p = rejection_sampling('P', dict(Awful=False, Bad=False, Boring=False, Dull=False, Effective=True, Enjoyable=True, Great=True, Hilarious=True), classifier, 10000)\n",
    "print(p[True])\n",
    "\n",
    "ans_dist = enumeration_ask('P', {'Awful': True, 'Bad': False, 'Boring': False, 'Dull': False, 'Effective': True, 'Enjoyable': True, 'Great': False, 'Hilarious': True}, classifier)\n",
    "print(ans_dist[True])\n",
    "p = rejection_sampling('P', dict(Awful=True, Bad=False, Boring=False, Dull=False, Effective=True, Enjoyable=True, Great=False, Hilarious=True), classifier, 10000)\n",
    "print(p[True])\n",
    "\n",
    "ans_dist = enumeration_ask('P', {'Awful': True, 'Bad': True, 'Boring': True, 'Dull': True, 'Effective': False, 'Enjoyable': False, 'Great': False, 'Hilarious': False}, classifier)\n",
    "print(ans_dist[False])\n",
    "p = rejection_sampling('P', dict(Awful=True, Bad=True, Boring=True, Dull=true, Effective=False, Enjoyable=False, Great=False, Hilarious=False), classifier, 10000)\n",
    "print(p[False])\n",
    "\n",
    "ans_dist = enumeration_ask('P', {'Awful': True, 'Bad': True, 'Boring': False, 'Dull': True, 'Effective': True, 'Enjoyable': False, 'Great': False, 'Hilarious': False}, classifier)\n",
    "print(ans_dist[False])\n",
    "p = rejection_sampling('P', dict(Awful=True, Bad=True, Boring=False, Dull=True, Effective=True, Enjoyable=False, Great=False, Hilarious=False), classifier, 10000)\n",
    "print(p[False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (Basic) -1 point\n",
    "\n",
    "\n",
    "The next three question explore hidden markov models (HMMs) and use the hmmlearn Python library. You can use the code for the weather example in the probabilistic reasoning over time notebook we covered in class as a template for writing your code. \n",
    "\n",
    "The problem used in inspired by the use of HMMs in bioinformatics. \n",
    "There are several simplifications made to make it reasonable as part of an assignment. DNA sequences can be considered strings over an alphabet of 4 symbols/nucleobases **A,C,T,G (adenine, cytosine, thymine, guanine**. Parts of a DNA sequence are dense with C and G and other parts are sparse with C and G and it is of interest to biologists to identify these regions. \n",
    "\n",
    "We will model the CG-dense **(CGD)** and **CG-sparse** (CGS) as hidden states and the nucleobases are the observations. Through experimental data we have the following information: \n",
    "\n",
    "1. The transition probability from CGR to CGP is 0.37 and the probability of staying in CGR is 0.63. The transition probability from CGP to CGR is similarly 0.37 with 0.63 being the probability of staying in CGP. \n",
    "\n",
    "2. The observation probabilities of CGR regions are: A: 0.15, C:0.35, G: 0.35, and T:0.15. The observation probabilities of CGP regions are: A: 0.40, C: 0.10, G: 0.10, T: 0.40 \n",
    "\n",
    "3. You can assume that the initial state probabilities are the same (0.5) \n",
    "\n",
    "4. For visualization of the DNA sequences use the following color mapping: A: red, C: green, T: blue, G: yellow, and for CGD: black \n",
    "and CGR: white \n",
    "\n",
    "\n",
    "Define this HMM model using the **hmmlearn** conventions. Then use the created model to generate a sequence of 1000 samples (i.e both hidden states and corresponding observations). Use the colors above \n",
    "to visualize the sequence of samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAABlCAYAAABZcXdQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKx0lEQVR4nO3dbaxlV13H8e+PTplKq522U0enT1NCUzLBIE2FqqhYTGEKobxoAmhCU5uMJCJgTLTIC+CFRgyxYiSYBipP2gIDSoMPpVQCb2xhBhUL09qZinT6NH2aPhCVlv59cdapt5d17znn3nvuuffO95Oc3LPXXmfttfeadc5v9j4PqSokSZL0bM+ZdQckSZLWIkOSJElShyFJkiSpw5AkSZLUYUiSJEnqMCRJkiR1GJIkSZI6DEmSZibJe5J8coL6r0hyaJp9kqQhQ5IkSVKHIUnSqkjye0nuTvJ4ktuTvAb4feANSZ5I8m+t3uVJ9rd6dyb5jVZ+PPAPwPZW/4kk25M8J8mVSQ4meSjJp5Oc3B5zXJJPtvIjSb6eZNusjoGk9cWQJGnqkpwLvBX4mar6UeBVwG3AHwKfqqoTqurFrfph4LXAjwGXA1clOa+qvgfsAu5p9U+oqnuA3wJeD/wSsB14BPhga+sy4ETgDOAU4C3Af097fyVtDIYkSavhB8BmYGeSY6vqO1V1sFexqv6uqg7WwFeALwK/sEjbbwHeVVWHqup/gfcAlybZBDzJIBy9oKp+UFX7quqxldwxSRuXIUnS1FXVAeAdDALM4STXJdneq5tkV5Kbkzyc5AhwMbB1kebPAv6mXU47AuxnEMq2AZ8AbgCuS3JPkj9OcuwK7ZakDc6QJGlVVNVfV9XLGYSaAt7X/j4jyWbgs8D7gW1VtQX4eyDDZjpN3wXsqqotc27HVdXdVfVkVb23qnYCP8fgMt6bp7F/kjYeQ5KkqUtybpILWwj6HwbvC3oauB/YkWT4XPRcBpflHgCeSrILuGhOU/cDpyQ5cU7ZXwB/kOSstq1Tk1zS7v9ykp9KcgzwGIPLb09PbUclbSiGJEmrYTPwR8CDwH3AjwPvBD7T1j+U5BtV9TjwNuDTDN6A/avA9cNGquo24FrgznZ5bTvwgVbni0keB24GXtYe8hPAHgYBaT/wFQaX4CRppFT1zl5LkiQd3TyTJEmS1GFIkiRJ6jAkSZIkdRiSJEmSOjaNqpDkGgbfLXK4ql40TqNbt26tHTt2LLNrkiRJ07dv374Hq+rU+eUjQxLwUeDPgY+Pu7EdO3awd+/e8XsnSZI0I0n+q1c+8nJbVX0VeHjFeyRJkrSGjXMmaSxJdgO7Ac4888yVanac7S64bv53QC1Wd6HHDR+zWFvDdfPrzt9eVY3dh15/5rbfuz/Odua2tVA/59cd6u3zYm3N72fP3Mf2ttPbn97yQsYZp4W2O9+o7Sw27gvVHWWxsRl1TEb1YzmG+zDJnBr1b2HY7iQWmguLjfVCx2WScVtoDBf69zxu3Z7lPI8sVHdYvlC/hnUW6sc4x2x++8t5/l1sX+avH3eezt/GYsdiqX1ebP1Cc3axMRu3371+LGX+j3qdmHTdUl4bZ/1djiv2xu2qurqqzq+q80899Ycu60mSJK0rfrpNkiSpw5AkSZLUMTIkJbkW+Gfg3CSHklwx/W5JkiTN1sg3blfVm1ajI5IkSWuJl9skSZI6DEmSJEkdhiRJkqQOQ5IkSVKHIUmSJKnDkCRJktRhSJIkSeowJEmSJHUYkiRJkjoMSZIkSR2GJEmSpA5DkiRJUochSZIkqcOQJEmS1GFIkiRJ6jAkSZIkdRiSJEmSOgxJkiRJHYYkSZKkDkOSJElShyFJkiSpw5AkSZLUYUiSJEnqMCRJkiR1GJIkSZI6DEmSJEkdhiRJkqQOQ5IkSVKHIUmSJKnDkCRJktRhSJIkSeowJEmSJHUYkiRJkjoMSZIkSR2GJEmSpA5DkiRJUochSZIkqcOQJEmS1GFIkiRJ6jAkSZIkdRiSJEmSOgxJkiRJHYYkSZKkjrFCUpJXJ7k9yYEkV067U5IkSbM2MiQlOQb4ILAL2Am8KcnOaXdMkiRplsY5k/RS4EBV3VlV3weuAy6ZbrckSZJma9MYdU4D7pqzfAh42fxKSXYDu9viE0luX373FrUVeHCxCkmW1HDvcYu1NX/dQnVXqj9zl5fT11H9WeI+PzMu425r0n6Me7wnbWfSx4+7ftIxWkpfRixvBR5c7jZH9WHcuivZj4XanWSslzpuKzDeW5Ms+hw2bvuT1h2WL3VejrvvSxnrpT5/jrNPY7Q/8nVlOX1brGxU/5f7urKSr4dLXbeUfUiypDFZgrN6heOEpLFU1dXA1SvV3ihJ9lbV+au1PY3HcVl7HJO1yXFZexyTtWfWYzLO5ba7gTPmLJ/eyiRJkjascULS14Fzkpyd5LnAG4Hrp9stSZKk2Rp5ua2qnkryVuAG4Bjgmqr61tR7NtqqXdrTRByXtccxWZscl7XHMVl7ZjomqapZbl+SJGlN8hu3JUmSOgxJkiRJHesyJPkzKbOR5IwkX07y7STfSvL2Vn5ykhuT3NH+ntTKk+TP2jh9M8l5s92DjSvJMUn+JckX2vLZSW5px/5T7UMXJNnclg+09Ttm2vENLMmWJHuS3JZkf5Kfda7MVpLfbs9dtya5NslxzpXVl+SaJIeT3DqnbOK5keSyVv+OJJdNo6/rLiTFn0mZpaeA36mqncAFwG+2Y38lcFNVnQPc1JZhMEbntNtu4EOr3+WjxtuB/XOW3wdcVVUvAB4BrmjlVwCPtPKrWj1NxweAf6yqFwIvZjA+zpUZSXIa8Dbg/Kp6EYMPIr0R58osfBR49byyieZGkpOBdzP4cuuXAu8eBquVtO5CEv5MysxU1b1V9Y12/3EGT/qnMTj+H2vVPga8vt2/BPh4DdwMbEnyk6vb640vyenAa4APt+UAFwJ7WpX5YzIcqz3AK7PUr+LVgpKcCPwi8BGAqvp+VR3BuTJrm4AfSbIJeB5wL86VVVdVXwUenlc86dx4FXBjVT1cVY8AN/LDwWvZ1mNI6v1Mymkz6stRq516fglwC7Ctqu5tq+4DtrX7jtXq+FPgd4Gn2/IpwJGqeqotzz3uz4xJW/9oq6+VdTbwAPCX7TLoh5Mcj3NlZqrqbuD9wHcZhKNHgX04V9aKSefGqsyZ9RiSNGNJTgA+C7yjqh6bu64G3ynh90qskiSvBQ5X1b5Z90XPsgk4D/hQVb0E+B7/f/kAcK6stnYp5hIGAXY7cDxTOPOg5VtLc2M9hiR/JmWGkhzLICD9VVV9rhXfP7w00P4ebuWO1fT9PPC6JN9hcOn5QgbvhdnSLinAs4/7M2PS1p8IPLSaHT5KHAIOVdUtbXkPg9DkXJmdXwH+s6oeqKongc8xmD/OlbVh0rmxKnNmPYYkfyZlRtr1+I8A+6vqT+asuh4YfrLgMuDzc8rf3D6dcAHw6JzTqVoBVfXOqjq9qnYwmAv/VFW/BnwZuLRVmz8mw7G6tNVfE/9j20iq6j7griTntqJXAt/GuTJL3wUuSPK89lw2HBPnytow6dy4AbgoyUntLOFFrWxlVdW6uwEXA/8BHATeNev+HC034OUMToF+E/jXdruYwXX6m4A7gC8BJ7f6YfBJxIPAvzP4VMnM92Oj3oBXAF9o958PfA04AHwG2NzKj2vLB9r658+63xv1Bvw0sLfNl78FTnKuzHxM3gvcBtwKfALY7FyZyThcy+B9YU8yOOt6xVLmBvDrbXwOAJdPo6/+LIkkSVLHerzcJkmSNHWGJEmSpA5DkiRJUochSZIkqcOQJEmS1GFIkiRJ6jAkSZIkdfwf4qbMCYXU2uIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "states = ['CGD', 'CGS']\n",
    "observations = ['A', 'C', 'G', 'T']\n",
    "start_probs = np.array([0.5, 0.5])\n",
    "transmat1 = np.array([[0.85, 0.15], [0.15, 0.85]])\n",
    "transmat2 = np.array([[0.15, 0.35, 0.35, 0.15], [0.40, 0.10, 0.10, 0.40]])\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_probs\n",
    "model.transmat_ = transmat1\n",
    "model.emissionprob_ = transmat2\n",
    "\n",
    "X, Z = model.sample(1000)\n",
    "\n",
    "def plot_DNA(samples, state2color, title): \n",
    "    colors = [state2color[x] for x in samples]\n",
    "    x = np.arange(0, len(colors))\n",
    "    y = np.ones(len(colors))\n",
    "    plt.figure(figsize=(10,1))\n",
    "    plt.bar(x, y, color=colors, width=1)\n",
    "    plt.title(title)\n",
    "    \n",
    "\n",
    "# we have to re-define state2color and obj2color as the hmm-learn \n",
    "# package just outputs numbers for the states \n",
    "state2color = {} \n",
    "state2color[0] = 'black'\n",
    "state2color[1] = 'white'\n",
    "plot_DNA(Z, state2color, 'states')\n",
    "\n",
    "samples = [item for sublist in X for item in sublist]\n",
    "obj2color = {} \n",
    "obj2color[0] = 'red'\n",
    "obj2color[1] = 'green'\n",
    "obj2color[2] = 'yellow'\n",
    "obj2color[3] = 'blue'\n",
    "plot_DNA(samples, obj2color, 'observations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 (Expected) -1 point\n",
    "\n",
    "Generate 10000 samples using the defined hmm for generating DNA sequences. Learn the HMM in an unsupervised fashion similarly to what we did with the weather example i.e only use the observation samples not the \"hidden\" states. Constrast the original HMM to the HMM estimated from the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix\n",
      "Estimated model:\n",
      "[[0.84739626 0.15260374]\n",
      " [0.17396612 0.82603388]]\n",
      "Original model:\n",
      "[[0.85 0.15]\n",
      " [0.15 0.85]]\n",
      "Emission probabilities\n",
      "Estimated model\n",
      "[[0.15532679 0.35027158 0.36222884 0.13217279]\n",
      " [0.39375166 0.09174497 0.09459657 0.41990679]]\n",
      "Original model\n",
      "[[0.15 0.35 0.35 0.15]\n",
      " [0.4  0.1  0.1  0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "# generate the samples \n",
    "X, Z = model.sample(10000)\n",
    "# learn a new model \n",
    "estimated_model = hmm.MultinomialHMM(n_components=2, n_iter=1000000).fit(X)\n",
    "print(\"Transition matrix\")\n",
    "print(\"Estimated model:\")\n",
    "print(estimated_model.transmat_)\n",
    "print(\"Original model:\")\n",
    "print(model.transmat_)\n",
    "print(\"Emission probabilities\")\n",
    "print(\"Estimated model\")\n",
    "print(estimated_model.emissionprob_)\n",
    "print(\"Original model\")\n",
    "print(model.emissionprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 (Expected) -1 point\n",
    "\n",
    "Write a function called **classification_accuracy** that takes as input \n",
    "two arrays or lists of states and returns the number of states that are the same in both lists as a percentage. \n",
    "\n",
    "Consider the original sequences of states of the generated samples \n",
    "as ground truth. Then use the estimated model from the previous \n",
    "question to generate predicted states from the observation samples. \n",
    "That is the maximum likelihood sequence estimation problem. \n",
    "Note that the predicted states might be inverted compared to the original and you need to deal with that in your code (see the class notebook for details). Now compute the accuracy between the predicted \n",
    "sequence of states and the ground truth sequence of states. \n",
    "This is similar to the visual comparison of the original and predicted states in the provided notebook but using a quantified \n",
    "metric rather than a visualization. \n",
    "\n",
    "Now replace the transition model of the original HMM with a transition model that is all 0.5 i.e there is no transition information. Effectively this disregards any temporal dependenices and each time step is decided independently. In fact it corresponds to a Naive Bayes classifier with a single feature which is the nucleobase observation. \n",
    "\n",
    "What is the classification accuracy in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmm accuracy : 79.57%\n",
      "Naive Bayes Accuracy : 51.09%\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "def classification_accuracy(array1, array2):\n",
    "    count = 0\n",
    "    i = 0\n",
    "    for item in array1:\n",
    "        if item == array2[i]:\n",
    "            count += 1\n",
    "        i += 1\n",
    "    percentage = (count/len(array1))*100\n",
    "    return percentage\n",
    "    \n",
    "Z2 = estimated_model.predict(X)\n",
    "acc = classification_accuracy(Z,Z2)\n",
    "\n",
    "def invert(array):\n",
    "    for i in range(0,len(array)):\n",
    "        if array[i] == 0:\n",
    "            array[i] = 1\n",
    "        else:\n",
    "            array[i] = 0\n",
    "    return array\n",
    "            \n",
    "Z2inv = invert(Z2)\n",
    "invacc = classification_accuracy(Z,Z2inv)\n",
    "\n",
    "transmat3 = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "model2 = hmm.MultinomialHMM(n_components=2)\n",
    "model2.startprob_ = start_probs\n",
    "model2.transmat_ = transmat3\n",
    "model2.emissionprob_ = transmat2\n",
    "Xnew, Znew = model2.sample(10000)\n",
    "estimated_model2 = hmm.MultinomialHMM(n_components=2, n_iter=10000).fit(Xnew)\n",
    "Znew = estimated_model2.predict(Xnew)\n",
    "naiveacc = classification_accuracy(Znew,Z2)\n",
    "\n",
    "if invacc > acc:\n",
    "    print('Flipped state accuracy' + \" \" + \":\" + \" \" + str(invacc) + \"%\")\n",
    "else:\n",
    "    print('Hmm accuracy' + \" \" + \":\" + \" \" + str(acc) + \"%\")\n",
    "    \n",
    "print('Naive Bayes Accuracy' + \" \" + \":\" + \" \" + str(naiveacc) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 (Advanced) -1 point\n",
    "\n",
    "This question is a bit more open ended, will require some creativity and extra work. Consider the following problem: during your day your cell phone collects location data in terms of x,y coordinates. You do different activities such as going to university, eating, going to the gym. These activities take place in particular locations such as Restaurant A and Restaurant B or Gym A, Gym B and each particular location can be thought of as a two-dimensional Gaussian distribution of location points. If you consider the activity as the hidden state and the location as the observation you have a Hidden Markov Model. Because activities take place in multiple locations you can model this as a Gaussian Mixture Model (GMM). Each Gaussian will be multivariate 2D Gaussian distribution characterized by two means and and a 2 by 2 covariance matrix.\n",
    "\n",
    "Consider a hypothetical scenario with 3 activities (eat, study, exercise) and 3 locations (GMM components) for each activity. You will need to do some reading about how GMMs work. You can come up \n",
    "with reasonable estimates for the associated parameters. \n",
    "\n",
    "Basically the goal is the follow the format of the Markov Chain and HMM notebook and create appropriate visualizations using this problem.\n",
    "\n",
    "Visualize on a 2D plane using circles the different locations and corresponding mixture components\n",
    "Generate a dataset using a Hidden Markov Model of the problem\n",
    "Visualize the dataset on a 2D plane\n",
    "Show how you can learn the parameters of this HMM using https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GMMHMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9 (Basic) - 1 point\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this question is to get some familiarity with scikit-learn: https://scikit-learn.org/stable/\n",
    "\n",
    "Replicate movie review classification from the previous assignment using bernoulli Naive Bayes in sklearn. This is relatively straightforward you simply need to create appropriate binary feature matrix and labels. Report on the classification accuracy and confusion matrix for that problem using 3-fold cross-validation. \n",
    "You will need to consult the execllent sklearn documentation for details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190 142]\n",
      " [ 89 246]]\n",
      "[[205 124]\n",
      " [ 78 260]]\n",
      "[[198 141]\n",
      " [ 82 245]]\n",
      "67.3010482371427% accuracy\n",
      "Final confusion matrix\n",
      "[[593 407]\n",
      " [249 751]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import fnmatch as fn\n",
    "\n",
    "path = \"./txt_sentoken\"\n",
    "data = []\n",
    "polarity = []\n",
    "pols = ['neg', 'pos']\n",
    "negvectors = []\n",
    "posvectors = []\n",
    "sumnegvectors = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "sumposvectors = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "file_filter = \"*.txt\"\n",
    "\n",
    "for index, pol in enumerate(pols):\n",
    "    files = fn.filter(os.listdir(os.path.join(path, pol)), file_filter)\n",
    "    for file in files:\n",
    "        masterlist = []\n",
    "        with open(os.path.join(path, pol, file)) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                list = line.split(\" \")\n",
    "                masterlist += list\n",
    "            data.append(masterlist)\n",
    "            polarity.append(index)\n",
    "words = ['awful', 'bad', 'boring', 'dull', 'effective', 'enjoyable', 'great', 'hilarious']\n",
    "for i, text in enumerate(data):\n",
    "    vector = [0, 0, 0, 0, 0, 0 ,0, 0]\n",
    "    for wordi, word in enumerate(words):\n",
    "        for item in text:\n",
    "            if word == item:\n",
    "                vector[wordi] = 1\n",
    "    if polarity[i] == 0: #negative review\n",
    "        negvectors.append(vector)\n",
    "        for j, item in enumerate(vector):\n",
    "            if vector[j] == 1:\n",
    "                sumnegvectors[j] += 1\n",
    "    else:\n",
    "        posvectors.append(vector) #pos review\n",
    "        for j, item in enumerate(vector):\n",
    "            if vector[j] == 1:\n",
    "                sumposvectors[j] += 1\n",
    "\n",
    "X = []\n",
    "for item in negvectors:\n",
    "    X.append(item)\n",
    "for item in posvectors:\n",
    "    X.append(item)\n",
    "    \n",
    "y = []\n",
    "for i in range(0,2000):\n",
    "    if i <= 999:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "true_fold = np.array([])\n",
    "predicted_fold = np.array([])\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "classifier = BernoulliNB()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    classifier_fold = classifier.fit(X_train, y_train)\n",
    "    y_predict = classifier.predict(X_test)\n",
    "    scores = np.append(scores, accuracy_score(y_predict, y_test))\n",
    "    true_fold = np.append(true_fold, y_test)\n",
    "    predicted_fold = np.append(predicted_fold, y_predict)\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "    \n",
    "print(str(scores.mean()*100) + \"%\" + \" \" + \"accuracy\")\n",
    "print(\"Final confusion matrix\")\n",
    "print(confusion_matrix(true_fold, predicted_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10 (Expected) - 1 point \n",
    "\n",
    "The goal of this question is to give you some familiarity with having continuous features and comparing different classifiers. For this question use the breast cancer dataset from sklearn: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "\n",
    "Train and compare three classifiers using this dataset using 3-fold \n",
    "cross-validation to calculate the classification accuracy and classification report: \n",
    "\n",
    "1. The Gaussian Naive Bayes classifier (with default parameters) \n",
    "(from sklearn.naive_bayes import GaussianNB) \n",
    "2. Linear support vector machine (with default parameters) \n",
    "(from sklearn.svm import LinearSVC) \n",
    "3. Decision tree (with default parameters) \n",
    "(from sklearn.tree import DecisionTreeClassifier)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Report\n",
      "93.67492806089297% accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.95      0.89      0.92       212\n",
      "      benign       0.94      0.97      0.95       357\n",
      "\n",
      "    accuracy                           0.94       569\n",
      "   macro avg       0.94      0.93      0.94       569\n",
      "weighted avg       0.94      0.94      0.94       569\n",
      "\n",
      "LinearSVC Report\n",
      "91.21600297038893% accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.61      1.00      0.76       212\n",
      "      benign       1.00      0.62      0.76       357\n",
      "\n",
      "    accuracy                           0.76       569\n",
      "   macro avg       0.80      0.81      0.76       569\n",
      "weighted avg       0.85      0.76      0.76       569\n",
      "\n",
      "Decision Tree Report\n",
      "89.09774436090227% accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       1.00      1.00      1.00       212\n",
      "      benign       1.00      1.00      1.00       357\n",
      "\n",
      "    accuracy                           1.00       569\n",
      "   macro avg       1.00      1.00      1.00       569\n",
      "weighted avg       1.00      1.00      1.00       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "classifier1 = GaussianNB()\n",
    "classifier1.fit(X, y)\n",
    "scores1 = cross_val_score(classifier1, X, y, cv=3)\n",
    "\n",
    "classifier2 = LinearSVC()\n",
    "classifier2.fit(X, y)\n",
    "scores2 = cross_val_score(classifier2, X, y, cv=3)\n",
    "\n",
    "classifier3 = DecisionTreeClassifier()\n",
    "classifier3.fit(X,y)\n",
    "scores3 = cross_val_score(classifier3, X, y, cv=3)\n",
    "\n",
    "print(\"GaussianNB Report\")\n",
    "print(str(scores1.mean()*100) + \"%\" + \" \" + \"accuracy\")\n",
    "y_pred1 = classifier1.predict(X)\n",
    "report1 = classification_report(y, y_pred1, target_names=data.target_names)\n",
    "print(report1)\n",
    "\n",
    "print(\"LinearSVC Report\")\n",
    "print(str(scores2.mean()*100) + \"%\" + \" \" + \"accuracy\")\n",
    "y_pred2 = classifier2.predict(X)\n",
    "report2 = classification_report(y, y_pred2, target_names=data.target_names)\n",
    "print(report2)\n",
    "\n",
    "print(\"Decision Tree Report\")\n",
    "print(str(scores3.mean()*100) + \"%\" + \" \" + \"accuracy\")\n",
    "y_pred3 = classifier3.predict(X)\n",
    "report3 = classification_report(y, y_pred3, target_names=data.target_names)\n",
    "print(report3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
